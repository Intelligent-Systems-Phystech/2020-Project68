\documentclass[12pt, twoside]{article}
\usepackage{jmlda}
\newcommand{\hdir}{.}

\pgfplotsset{compat=1.15}

\begin{document}

\title
    [Метаобучение тематических моделей классификации] % краткое название; не нужно, если полное название влезает в~колонтитул
    {Метаобучение тематических моделей классификации}
\author
    [А.\,С.~Ватолин] % список авторов (не более трех) для колонтитула; не нужен, если основной список влезает в колонтитул
    {А.\,С.~Ватолин, Ю.\,А.~Сердюк, К.\,В.~Воронцов} % основной список авторов, выводимый в оглавление
    [А.\,С.~Ватолин$^1$, Ю.\,А.~Сердюк$^2$, К.\,В.~Воронцов$^1$] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
\email
    {vatolinalex@gmail.com; masyes@mail.com;  vokov@forecsys.ru}
% \thanks
    % {Работа выполнена при
    %  %частичной
	%  финансовой поддержке РФФИ, проекты \No\ \No 00-00-00000 и 00-00-00001.}
	% {Задачу поставил: Воронцов~К.\,В.
    %     Консультант: Янина~А.\,О.}
\organization
    {$^1$ Московский физико-технический институт, Москва, Россия}
\abstract
    {Одним из возможных применений вероятностной тематической модели является построение модели не только для текста, но и для имеющихся метаданных (модальностей). Это позволяет более точно определять темы документов, а также предсказывать пропущенные метаданные по имеющимся. Каждая модальность имеет свой вес, который задается вручную и отражает меру влияния данной модальности на темы документов.
    В данной работе исследуются эвристики для начальной инициализации весов модальностей. Получение эвристики позволит
    полностью отказаться от перебора весов модальностей по сетке или же уменьшить количество вариантов перебора.
    Для оценки весов используется мера взаимной информации.
	
	\bigskip
	\noindent
	\textbf{Ключевые слова}: \emph {вероятностное тематическое моделирование; Мультимодальное тематическое моделирование; 
	BigARTM}
}

%данные поля заполняются редакцией журнала
% \doi{10.21469/22233792}
% \receivedRus{01.01.2017}
% \receivedEng{January 01, 2017}

\maketitle
\linenumbers

\section{Введение}
Вероятностное тематическое моделирование - способ построения модели текстовых документов, которая определяет к какой теме
относится каждый документ и какие слова образуют тему. Тематическое моделирование применяется в информационном поиске
\cite{tm_recomedation}, для классификации \cite{rubin2012statistical} и суммаризации текстов \cite{artm_summarization}, 
а также для ранжирования статей \cite{ranktopic}. Одним из продвинутых инструментов, который реализует все из перечисленных выше инструментов, является библиотека BigARTM \cite{vorontsov/artm_book}. Она обладает обширным набором параметров для настройки модели, а также различными резуляризаторами.


В данной статье внимание сконцентрировано на задаче классификации, а именно предлагаются эвристики для оптимальной инициализации весов модальностей в тематической модели. Вводится предположение о том, по статистикам исходной коллекции текстов можно вывести оценку 
Текущие подходы к выбору весов модальности сводятся к двум методам: перебор параметров по сетке или задание константного имперического значения. В работе \cite{vorontsov/blog_search} определение весов осуществляется перебором параметров по сетке методом проб и ошибок. Выбор лучшего набора парамеров осуществлялся по критериям перплекции, разреженности и качества тематического поиска. Также в работе \cite{vorontsov/transactions} предлагается ввести следующее правило: дополнительные модальности не должны увеличивать перпрелксию основной модальности. Введение этого правила не избавляет от перебора по сетке, а лишь изменяет правила выбора лучшего значения для весов. В статье \cite{vorontsov/hierarchical} веса модальностей задаются вручную из соображений важности каждой из модальностей для модели.

Одной из близких к данной задаче является настройка весов регуляризаторов вероятностной тематической модели \cite{vorontsov/artm_book}. Так как задача тематического моделирования является многокритериальной, то одновременно при обучении модели может использоваться несколько регуляризаторов, сбалансированных с помощью весов. В ходе экспериментов было установлено, что весов регуляризаторов зависят от параметров выборки, таких как размер коллекции, мощность словаря, средняя длина документов. Для избавления от необходимости перенастройки весов при изменении парамеров коллекции вводятся относительные коэффициенты регуляризации.

Подбор параметров осуществляется на выборках R8, R52, AG's news, Ohsumed, 20NG, DBPedia, IMDb, Amazon 2, Yelp 5, Sogou News. % TODO: поправить, если список изменится
% После аннотации, но перед первым разделом, располагается введение, включающее в себя
% описание предметной области, обоснование актуальности задачи,
% краткий обзор известных результатов.

\section{Постановка задачи}
Пусть задано $\mathfrak{D} = \left\{D_i\right\}_{i=1}^{N}$ - множество (коллекция) текстовых документов. Каждая коллекция $D$ состоит из документов $d$. $T$ - множество тем, $M$ - множество модальностей. $W^1,...,W^m$ - словари термов, соответствующие модальностям $m \in M$. Термами могут быть слова, нормальные формы слов, словосочетания и т.д. в зависимости от предобработки исходных данных. Каждый документ $D \in \mathfrak{D}$ представлен в виде последовательности $n_d$ термов $w_1,...,w_{n_d} \in W^m \ \forall m \in M$, где каждому терму ставится в соответствие число его вхождений $n_{dw}$. 


При построении мультимодальной тематической модели используются следующие обозначения: $p(w|t)$ - вероятность появления терма $w$ в теме $t$, $p(t|d)$ - вероятность появления темы $t$ в документе $d$. 

Тогда вероятность появления терма произвольной модальности в документе может быть выражена следующим образом:
\begin{equation}
	p(w|d) = \sum\limits_{t \in T}p(w|t)p(t|d) = \sum\limits_{t \in T}\phi_{wt}\theta_{td}, \ w \in W_m, d \in D
\end{equation}
Для каждой модальности $m$ ставится в соответствие матрица $\Phi_m = (\phi_{wt})_{W_m \times T}$. При объединении матриц каждой модальности $\Phi_m$ в одну матрицу образуется $W \times T$-матрица $\Phi$.

Мультимодальная модель строится путем максимизации взвешенной суммы логарифмов правдоподобия модальностей и регуляризаторов.
\begin{equation}
    \sum\limits_{m \in M} \tau_m \sum\limits_{d \in D} \sum\limits_{w \in W_m} n_{dw} \ln \sum\limits_{t \in T} \phi_{wt}\theta_{td} + R(\Phi, \Theta) \rightarrow  \max_{\Phi, \Theta} % bug in formula
\end{equation}

Предсказание делается следующим образом:
\begin{equation}
    p(c|d) = \sum_{t \in T} \phi_{ct}\theta_{td} % учет мультимодальности
\end{equation}

Документ $d$ относится к классу $c$, если $p(c|d) >= \gamma_c$


Качество классификации измеряется с помощью метрики $F1$ с микро усреднением
\begin{equation}
    F1_micro = \frac{2}{C} \sum\limits_{c \in C} \cdot \frac{precision_c \cdot recall_c}{precision_c + recall_c}
\end{equation}

% предсказание
% f1
% grid search http://www.machinelearning.ru/wiki/images/5/55/Minaev2011ms.pdf
% http://www.machinelearning.ru/wiki/images/6/6d/Voron-ML-1.pdf


 
\paragraph{Название параграфа}
Разделы и~параграфы, за исключением списков литературы, нумеруются.

\section{Заключение}
Желательно, чтобы этот раздел был, причём он не~должен дословно повторять аннотацию.
Обычно здесь отмечают, каких результатов удалось добиться, какие проблемы остались открытыми.

\bibliographystyle{plain}
\bibliography{literature}

\end{document}
